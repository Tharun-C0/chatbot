{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPH3uOeazwDWa1jz6F99CnQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tharun-C0/chatbot/blob/main/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxqDnjh5L3lV"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers torch accelerate gradio sympy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import re\n",
        "import gradio as gr\n",
        "import sympy as sp\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n"
      ],
      "metadata": {
        "id": "-ivcOjwT3p3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"google/flan-t5-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "id": "Z3k8bCcq3sJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_simple_math(expr):\n",
        "    return bool(re.fullmatch(r\"[0-9+\\-*/(). ]+\", expr))\n"
      ],
      "metadata": {
        "id": "Xwg_62UD3tZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def solve_math_exact(expr):\n",
        "    try:\n",
        "        result = sp.sympify(expr, evaluate=True)\n",
        "        return result\n",
        "    except:\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "v1rsfUbz3xDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def explain_math(expr, result):\n",
        "    prompt = f\"\"\"\n",
        "    Explain how to solve the following math problem step by step.\n",
        "    Do NOT calculate the final answer.\n",
        "    The final answer is already provided.\n",
        "\n",
        "    Problem: {expr}\n",
        "    Final Answer: {result}\n",
        "\n",
        "    Explanation:\n",
        "    \"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=160,\n",
        "        temperature=0.2,\n",
        "        do_sample=False\n",
        "    )\n",
        "\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "hB0n0qOa3yMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_definition_question(text):\n",
        "    keywords = [\n",
        "        \"define\", \"definition\", \"what is\", \"explain\",\n",
        "        \"meaning of\", \"describe\", \"tell me about\"\n",
        "    ]\n",
        "    return any(k in text.lower() for k in keywords)\n"
      ],
      "metadata": {
        "id": "WP5RbuEg3zfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_history = []\n",
        "\n",
        "def chatbot_response(user_input):\n",
        "\n",
        "\n",
        "    if is_simple_math(user_input):\n",
        "        exact_result = solve_math_exact(user_input)\n",
        "\n",
        "        if exact_result is not None:\n",
        "            explanation = explain_math(user_input, exact_result)\n",
        "\n",
        "            return (\n",
        "                f\"ðŸ§® **Math Solution**\\n\\n\"\n",
        "                f\"Expression: {user_input}\\n\"\n",
        "                f\"Final Answer: {exact_result}\\n\\n\"\n",
        "                f\"Explanation:\\n{explanation}\"\n",
        "            )\n",
        "\n",
        "\n",
        "    if is_definition_question(user_input):\n",
        "        prompt = f\"\"\"\n",
        "        You are an expert computer science professor.\n",
        "        Give a clear, accurate, and well-structured explanation.\n",
        "        Use simple language and examples if needed.\n",
        "\n",
        "        Question: {user_input}\n",
        "        Answer:\n",
        "        \"\"\"\n",
        "    else:\n",
        "\n",
        "        context = \" \".join(conversation_history[-4:])\n",
        "        prompt = f\"\"\"\n",
        "        You are a friendly and intelligent AI assistant.\n",
        "        Maintain a natural conversation.\n",
        "\n",
        "        Conversation history:\n",
        "        {context}\n",
        "\n",
        "        User: {user_input}\n",
        "        Assistant:\n",
        "        \"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=200,\n",
        "        temperature=0.3,\n",
        "        do_sample=False\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    conversation_history.append(f\"User: {user_input}\")\n",
        "    conversation_history.append(f\"Assistant: {response}\")\n",
        "\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "OF91GyVd31G8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_ui(user_message, history):\n",
        "    bot_reply = chatbot_response(user_message)\n",
        "    history.append((user_message, bot_reply))\n",
        "    return history, \"\"\n"
      ],
      "metadata": {
        "id": "tBv5t_zx328Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"THARUN  Chatbot\")\n",
        "    gr.Markdown(\"Handles conversation, definitions, and **accurate step-by-step math reasoning**\")\n",
        "\n",
        "    chatbot = gr.Chatbot(height=450)\n",
        "    msg = gr.Textbox(placeholder=\"Ask anything (math, concepts, chat)...\")\n",
        "    clear = gr.Button(\"Clear Chat\")\n",
        "\n",
        "    msg.submit(chat_ui, [msg, chatbot], [chatbot, msg])\n",
        "    clear.click(lambda: [], None, chatbot)\n",
        "\n",
        "demo.launch(share=False)\n"
      ],
      "metadata": {
        "id": "bzbGZ8ff35RE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}